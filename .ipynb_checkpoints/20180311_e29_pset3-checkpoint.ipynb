{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Name Here:\n",
    "### Your TF Name:\n",
    "### Graduate or Undergraduate Credit: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is e29 PSET3\n",
    "##### DUE: March 28 2018\n",
    "##### Harvard University DCE CSCI-E29\n",
    "##### Instructor: Nenad Svrzikapa\n",
    "##### Staff: Joe Palin, Kaleigh Douglas, Lena Hajjar, Phil Lodine, Alan Xie\n",
    "### Instructions:\n",
    "\n",
    "PSET3 is designed to solidify your Pandas skills.  Before you get started please review and understand our Academic Integrity policy.  We understand that for some of your work you may have to use external sorces.  Please cite and be a good acedemic citizen. Remember that All homework assignments, quizzes and exams must be your independent work!\n",
    "\n",
    "Undergraduate Credit students may solve any 4 problems.\n",
    "\n",
    "Tips from staff:  \n",
    "\n",
    "1. Don't wait until the last day, our Psets require time and learning\n",
    "2. Piazza is there for you, we are a team and our goal is not to test you, we want to teach you! Please ask questions!\n",
    "3. Program a bit every day.  You can't learn these concepts by reading 2 chapters and attempting to solve the problems. Read a little, try the code, change the code,break the code, understand, learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: \n",
    "### Created by: Lena\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Fun at Wal-mart\n",
    "\n",
    "## Problem by Lena\n",
    "\n",
    "I know what exactly what you're thinking.<br>\n",
    "\"That Wal-mart problem set was just SO riveting.\"<br>\n",
    "Well, you're in luck, because we're using the Wal-mart dataset again! <br>\n",
    "We're going to make a few functions to find out information about Wal-marts and Supercenters in each state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset (literally the exact same information as from PSET2)\n",
    "You can find the data in the file walmart.csv (source: https://github.com/plotly/datasets) The file has over 2000 records of Wal-Marts and Supercenters that have opened in the US since 1962. The dataset has the following columns:\n",
    "<ul>\n",
    "<li><b>storenum:</b> The store number</li>\n",
    "<li><b>OPENDATE:</b> The opening date of the store in MM/DD/YYYY format</li>\n",
    "<li><b>date_super:</b> The date that the store was converted into a supercenter in MM/DD/YYYY format. There may be empty values if the store was not converted into a supercenter</li>\n",
    "<li><b>conversion:</b> The number of conversions from Wal-Mart to Supercenter that the store has gone through. </li>\n",
    "<li><b>STREETADDR:</b> The store's street address</li>\n",
    "<li><b>STRCITY:</b> The store's city</li>\n",
    "<li><b>STRSTATE:</b> The store's state</li>\n",
    "<li><b>ZIPCODE:</b> The store's zipcode</li>\n",
    "<li><b>type_store:</b> The type of store. Possible values are Wal-mart and SuperCenter</li>\n",
    "<li><b>LAT:</b> The store's latitude</li>\n",
    "<li><b>LON:</b> The store's longitude</li>\n",
    "<li><b>MONTH:</b> The month the store opened</li>\n",
    "<li><b>DAY:</b> The day the store opened</li>\n",
    "<li><b>YEAR:</b> The year the store opened</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Getting the data\n",
    "You should have downloaded walmart.csv from the assignment (or even the last assignment). In the next cell, load the csv into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import pivot_table\n",
    "### Part 2: Exploring the dataimport pandas as pd\n",
    "\n",
    "# load walmart.csv into a dataframe. Hint: use the read_csv function\n",
    "\n",
    "walmart = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PSET2, we did some exploration of the dataset. Print first few rows using the .head() function, just so we can get a refresher of the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the first few rows HINT: Use the .head() function \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Finding information by state\n",
    "It's good to have information about store openings in each state. In the next cell, find all the stores that are in Arkansas. walmart.STRSTATE == \"AR\" should be somewhere in your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "ARStores = \n",
    "\n",
    "#Let's briefly look at your result\n",
    "ARStores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now we have all the stores in Arkansas. Now, make two more dataframes: one with all of the Supercenters in Arkansas, and one with all of the Wal-marts in Arkansas. You should be using the \"type_store\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# find all supercenters\n",
    "ARSuper = \n",
    "\n",
    "# find all wal-marts\n",
    "ARWalmart = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I'm truly just interested in the YEAR information, so make 2 series of just the YEAR Columns for both Supercenters and Walmarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Find all YEARS that Supercenters Opened\n",
    "ARSuperYears = \n",
    "\n",
    "#Find all YEARS that Walmarts opened\n",
    "ARWalmartYears = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the years, let's find out when the first store opened, when the last store opened, and how many stores there are in total. You should use the .min(), .max() and .count() functions! Hint: you'll have to compare the results of min and max for the two Series to find out when the first and last store in the whole state opened up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "\n",
    "# find min of ARSuperYears\n",
    "ARSuperMin = \n",
    "\n",
    "# find min of ARWalmartYyears\n",
    "ARWalmartMin = \n",
    "\n",
    "# find max of ARSuperYears\n",
    "ARSuperMax = \n",
    "\n",
    "# find max of ARWalmartYyears\n",
    "ARWalmartMax = \n",
    "\n",
    "# find count of ARSuperYears\n",
    "ARSuperCount = \n",
    "\n",
    "# find min of ARWalmartYyears\n",
    "ARWalmartCount = \n",
    "\n",
    "# find First store tha opened in AR\n",
    "ARFirst = \n",
    "\n",
    "# find Last store tha opened in AR\n",
    "ARLast = \n",
    "\n",
    "# find total number of stores in AR\n",
    "ARCount = \n",
    "\n",
    "#print results\n",
    "print(\"The first Supercenter opened in \", ARSuperMin)\n",
    "\n",
    "print(\"The first Walmart opened in \", ARWalmartMin)\n",
    "\n",
    "print(\"The last Supercenter opened in \", ARSuperMax)\n",
    "\n",
    "print(\"The last Walmart opened in \", ARWalmartMax)\n",
    "\n",
    "print(\"The first store opened in \", ARFirst)\n",
    "\n",
    "print(\"The last store opened in \", ARLast)\n",
    "\n",
    "print(\"The total number of Supercenters  is \", ARSuperCount)\n",
    "\n",
    "print(\"The total number of Wal-marts  is \", ARWalmartCount)\n",
    "\n",
    "print(\"The total number of stores is \", ARCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to find all this information, write a function called StateStoreData that takes a state abbreviation as the argument, and it returns a dictionary of state, total_stores, total_supercenters, total_walmarts, first_store, and last_store. \n",
    "\n",
    "Note: depending on how you solve this problem, you may get a warning from python because of the indexing. You can suppress the warnings by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StateStoreData(state):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass \"AR\" into the funciton you just made to make sure you get the results as you did before you made the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that there are at least 3 Wal-marts within 20 miles of my home in MA, this dataset doesn't have any information about MA stores. Does this function fail if you try to pass \"MA\"? <br>\n",
    "<b>Answer HERE:</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Using a pivot table\n",
    "You can also solve this problem by using a pivot table. Make a pivot table with STRSTATE as an index, type_store as the columns, YEAR as value, and have the aggfuncitons be min, max, and count. <br>\n",
    "<b>Note</b>: There is no np.count funciton, but there is an np.count_nonzero function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "walmartpivot= \n",
    "walmartpivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, return the row with the Arkansas data.  You should be looking at the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your Code here\n",
    "\n",
    "ARPivot = \n",
    "ARPivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use iloc to return values of specific cells. To test this, return the value of the count of Wal-Marts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ARWalmartCount = \n",
    "print(ARWalmartCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a new function called StateStoreData2 that takes a state abbreviation as the argument, and it returns a dictionary of state, total_stores, total_supercenters, total_walmarts, first_store, and last_store. You must use the pivot table and iloc!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StateStoreData2(state):\n",
    "    # your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StateStoreData2(\"AR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: \n",
    "### Created by: Phil\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Voting Results by Method\n",
    "## PSet3 Problem by Phil\n",
    "\n",
    "As some of us may remember, the 2000 Presidential election was extremely close and hotly contested. Much of the drama occurred in the state of Florida, where many ballots had to be recounted by hand. As the recount progressed, the nation learned of some of the shortcomings of various schemes of ballot and voting machine design. Some of the factors that were investigated included the type of mechanism used to count votes, and the design of the ballot itself.\n",
    "\n",
    "In the Florida election, each county used one of five methods to count votes:\n",
    "* Optical scanning of a paper ballot marked by filling in an oval next to the candidate's name\n",
    "* Votamatic machines, which require the voter to use a stylus to press out a perforated rectangle (a \"chad\") in a standard IBM punch card\n",
    "* Datavote machines, which use a similar mechanism to that of the Votamatic\n",
    "* Hand counting of a ballot filled out manually by the voter\n",
    "* Lever machines, which mechanically count the positions on a set of levers flipped by the voter\n",
    "\n",
    "Hand counting and lever machines are older methods that are rarely used today. Recent trends have been away from punch-card ballots toward either optical scanning or direct entry on an electronic touch-screen. In 2000, Florida used almost entirely either optical or punch-card systems.\n",
    "\n",
    "One significant problem in counting votes occurs when a voter fills out a ballot incorrectly. In many local elections one can vote for two or more candidates for a given position, up to a maximum. (For example, you might see an instruction like \"Vote for any THREE\" next to a list of ten candidates for three open positions on the local school board. If a voter's ballot shows choices for only one or two candidates in a multiple-choice race, or shows no choice at all for a single-choice race like President, this may have been his or her intention; *OR* the ballot may have been incorrectly marked, or may have been counted incorrectly by a machine. This situation is called an \"undervote\". Similarly, the voter's ballot may show more than the allowed number of candidates, an \"overvote\".\n",
    "\n",
    "Another factor that can cause problems is whether the candidates' names are listed in a single column or in two columns. The two-column arrangement (the infamous \"butterfly ballot\") arranged the names to the left and right of a single column of hole-punch locations, accommodating the small width of the IBM punch card placed below the sheet that lists the candidates. This arrangement may make it more difficult to identify the correct location to punch, leading to ballots whose markings do not reflect the voter's intention. \n",
    "\n",
    "While undervotes and overvotes may not invalidate an entire ballot, how often they occur can tell us something about the relative reliability of different technologies and ballot designs.\n",
    "\n",
    "Examples of the various ballots used recently in the U.S. can be seen here: https://www.verifiedvoting.org/resources/voting-equipment/. Images of the one- and two-column ballot shapes can be seen here: www.nytimes.com/images/2001/11/12/politics/recount/index_BALLOT.html\n",
    "\n",
    "Let's look at some of the results compiled during the Florida 2000 recount. Note that there were several minor-party candidates besides Bush and Gore, but we'll focus on those two.\n",
    "The fl2000.csv file contains the vote counts for each candidate in each county, along with:\n",
    "* the type of technology used to count votes\n",
    "* the number of columns on the ballot\n",
    "* the number of undervotes and overvotes\n",
    "\n",
    "This data was compiled by Brett Presnell of the University of Florida Statistics Department.\n",
    "\n",
    "Let's load the file (available on Canvas) the usual way and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('fl2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... that first row looks odd. It looks like the data includes a helpful line at the top giving the data type of each column. This might be helpful for some users, but we have Pandas, which can figure this out itself. Not only that, but the presence of the strings \"int\" in every column but the first two cause Pandas to refuse to parse the actual integers as integers. To see this, we can list the data types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, this extra row is a problem. Look up the documentation for the read_csv() function and find out how to use the skiprows parameter to skip just this one row. (Hint: this parameter behaves differently depending on whether you supply a single integer or a list.) Once you've reloaded the dataset, show the first 5 lines again to verify your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dtypes method again to verify that the values are being interpreted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'technology' column of your DataFrame is a pandas Series that tells the machine type or other method used by each county. Use the value_counts() method of the Series object to show how many counties use each of the five technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first few rows again, we see many columns of very low counts that we don't want to have cluttering up the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of these by creating a new DataFrame that contains only the Bush, Gore, Browne, and Nader counts, along with the first five columns, which don't refer to candidates. Use the DataFrame's copy() method so that we can modify the new DataFrame later without generating the SettingWithCopyWarning message you might have seen. (Hint: this can be done in a single line of code by making a copy of a DataFrame composed of the first 9 columns of the original DataFrame; there are probably several ways to do this.) Assign the new DataFrame to the same variable *df* and show its first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can see the magnitude of the under- and overvotes compared to the total number of votes cast, let's add a column called **All** that contains the sums of the Bush, Gore, Browne, and Nader vote counts. (Hint: once you've identified the columns you want to sum, you can sum them horizontally using sum() method's *axis* parameter.) If you find yourself re-executing your code as you debug it, you may find it handier to use an explicit [start:end] column range rather than an open-ended [start:] range, since adding a column will change the number of columns in the DataFrame each time you execute the cell. Once you've added the new column, show the first few lines of the DataFrame again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df['All'] = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there! Let's add 2 columns called **pct_under** and **pct_over** that show the percentages of the under- and overcounts, relative to the 'All' column. (Hint: these two lines will each just involve taking a column of the DataFrame, multiplying by 100 and dividing by another column.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df['pct_over'] = \n",
    "# df['pct_under'] = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's build a pivot table that shows the total vote counts (the 'All' column) and the percentages of under- and overvotes across the entire state, broken down by voting machine technology and then ballot format (columns). The goal here is to see if anything jumps out at us that might indicate whether certain technologies or ballot types were more problematic than others. (Hints: your pivot table will have two indexes and three values. You will need to use different aggregation functions for the individual columns, since one is a sum and the other two are percentages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# df.pivot_table(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What preliminary conclusions might you draw from the various percentage values? It may be significant that the 2-column ballot format appears to higher percentages of under- and overvotes when used with any technology. On the other hand, the Datavote technology appears to have an even higher percentage of under- and overvotes with a 1-column ballot format. (There's no right or wrong answer here. We're not going to attempt any more sophisticated statistical analysis here -- we've done enough work for one day!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: an annotated version of this dataset, with some comments by its author, is available here: http://hci.stanford.edu/courses/cs448b/data/florida-voting/fl2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: \n",
    "### Created by: Kaleigh\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olympic Games - Summer and Winter\n",
    "\n",
    "In this problem, we are exploring the olympic games records from both the winter and summer olympics. (The winter.csv file is the same as the file we used in Problem Set # 2.)  \n",
    "Source: https://www.kaggle.com/the-guardian/olympic-games/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Datasets  \n",
    "\n",
    "- Load the \"winter.csv\" file as a pandas dataframe.  \n",
    "- Load the \"summer.csv\" file as a pandas dataframe.  \n",
    "- Display the **first 5** rows of the winter dataframe.  \n",
    "- Display the **first 5** rows of the summer dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "\n",
    "#winter = \n",
    "#summer = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Merge Data\n",
    "### 2.1) Add \"Season\" Column To Each Dataframe  \n",
    "- For your winter dataframe, add a column with the heading \"Season\" where every value is \"Winter\"\n",
    "- For your summer dataframe, add a column with the heading \"Season\" where every value is \"Summer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Merge The Winter And Summer Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "\n",
    "#olympics =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Clean Up Data\n",
    "### 3.1) Remove Rows with Null Values  \n",
    "In PSET 2, we confirmed there are no null values in the winter.csv dataset, however there are a few null values from the summer.csv dataset.  \n",
    "Drop all rows with null values in your dataframe from Part 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Remove Team Sport Bias  \n",
    "This dataset includes every medal awarded to every individual athlete.  In team events, each medal (gold, silver and bronze) are awarded to multiple athletes.  (For example, the winning ice hockey team would receive ~20 gold medals.) In order to focus on the number of events a country has medaled in, rather than the number of athletes that have received medals, we're going to remove all \"duplicate\" medals from the same event.  \n",
    "\n",
    "We'll do this by first dropping the 'Athlete' column and then removing all duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the 'Athlete' Column\n",
    "# Your Code Here\n",
    "\n",
    "\n",
    "\n",
    "# Drop Duplcate Rows\n",
    "# Your Code Here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) How Many Gold Medals Has Jamaica Won in Athletics?   \n",
    "### 4.1) Create a pivot table using the 'Country' and 'Sport' columns as indices and Medal (Gold, Silver, Bronze) as the table's columns.   The table's values will be the medal count for that particular country/sport/medal color.\n",
    "\n",
    "**Display the head of the pivot table.**  \n",
    "Note: Use the dataframe from Part 3.  \n",
    "Note: The order of the Gold, Silver, Bronze columns does not matter.  \n",
    "Hint: use fill_value = 0.  \n",
    "\n",
    "Hint: The first three rows of your pivot table should look something like the following:  \n",
    "\n",
    "    |        | Medal     | Bronze | Gold | Silver |  \n",
    "    |Country | Sport     |        |      |        |  \n",
    "    |AFG     | Taekwondo |   2    |   0  |    0   |  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Access the pivot table to find the total number of Gold Medals that Jamaica (country code 'JAM') has won in the 'Athletics' Sport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Medal Counts Per Olympic Season\n",
    "### 5.1) Create a pivot table using the 'Country' column as the table's index and the Seasons (Summer and Winter) as the table's columns.  The table's values will be the medal count for that particular country/season.\n",
    "\n",
    "**Display the head of the pivot table.**  \n",
    "Note: Use the dataframe from Part 3.  \n",
    "Hint: use fill_value = 0.  \n",
    "\n",
    "Hint: The first three rows of your pivot table should look something like the following:  \n",
    "\n",
    "    |Season  | Summer | Winter |\n",
    "    |Country |        |        | \n",
    "    |AFG     | 2      |   0    | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "### 5.2) Display a plot of the pivot table from part 5.1. The plot should be a stacked horizontal bar graph, with the medal count on the x axis and the country codes on the y axis.  (Don't worry about plot titles/labels).     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: \n",
    "### Created by: Alan\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Single-Game Win Model for the NBA (Part 1)\n",
    "\n",
    "### Problem by Alan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any basketball fans out here? (Personally, I'm more of an NFL kind of guy. How 'bout them Cowboys!) Nonetheless, get ready for some NBA excitement!\n",
    "\n",
    "This problem (and its follow-ups in future problem sets) will take us from 0 to 1 in building a rudimentary model that can predict the likelihood of a team winning any single game during the NBA regular season. \n",
    "\n",
    "This week, we'll primarily focus on data exploration through the use of pivot tables. We'll also reinforce some of the DataFrame slicing and indexing concepts that we learned about last week. \n",
    "\n",
    "#### Step 1: Scrape and clean the data\n",
    "\n",
    "Luckily, I've already gone to the trouble of collecting this data for you. (If you're curious, I collected the data by creating a local fork of [this existing API](https://github.com/jaebradley/basketball_reference_web_scraper) which scrapes the website [Basketball Reference](https://www.basketball-reference.com). Then, I rewrote the API to work again - the code on GitHub no longer functions and also relies on more outdated methods of web scraping such as the `urllib2` and `lxml` libraries instead of `requests` and `BeautifulSoup`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore the data\n",
    "\n",
    "This is an important step for us to eventually build a predictive model. We need to understand what our data looks like and what additional processing (if any) we might need to do in order to generate features for our model. I've provided you with three files: `2012_2017_box_scores.csv`, `2012_2017_schedules.csv`, and `2012_2017_season_player_stats.csv`. As you can see, the data provided is for the NBA regular season for a 6-year period between 2012 and 2017. (If you're wondering why the cut-off was somewhat arbitrarily decided to be 2012, that's because the 2011-2012 season was cut short by a player lockout.)\n",
    "\n",
    "Let's get started by reading in our data using `pd.read_csv()`. Note that you might end up with an extra column if you don't pass in a value for the optional parameter `index_col`. This is because these files were previously created by `pd.to_csv()`, which by default exports a DataFrame's index as a regular column. The `index_col` parameter will allow us to read that column as the index, and our DataFrame will look normal again. This is especially useful for datasets with irregular indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine each DataFrame and note in the below Markdown cell some initial observations. These can be some thoughts about the columns, the shape of the data, or the relationship that each DataFrame has with the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Box Scores:**\n",
    "\n",
    "**Schedules:**\n",
    "\n",
    "**Season Player Stats:**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important task before us is to try to identify key features that might be predictive for our model. But what exactly would our model look like? Here, it's helpful to identify what our X and Y variables are. \n",
    "\n",
    "One analogy we can draw is to movies. If we want to build a box office forecasting model for opening weekend gross, then it's pretty clear that our Y variable is a number like `opening_weekend_box_office` (in dollars). Our X variables then might be things like `budget` (in dollars), `opening_number_of_theaters`, `holiday_weekend` (Boolean), and `sequel` (Boolean). Note that a qualitative variable like `director` might not immediately be useful for our model, as opposed to quantitative variables derived from `director` such as `average_director_lifetime_gross`. When we actually train our model, each observation in our train/test set is a single movie.\n",
    "\n",
    "In the Markdown cell below, identify the X and Y variables for a model that aims to predict whether a home team will win an NBA regular season basketball game. The list of X variables does not have to be exhaustive but try to think of at least 5. You can refer to the Box Scores DataFrame for inspiration. We will tackle this topic of feature generation in greater detail in the future, so don't worry too much about it right now.\n",
    "\n",
    "_Note: This is an important exercise because practical ML problems will require that you devote extensive thought to feature generation at the data exploration and processing step. In fact, proprietary features and data are often considered the \"secret sauce\" of most ML companies because open-source tools such as scikit-learn and Keras (combined with cheap AWS computing) have enabled anyone to perform sophisticated predictive analysis that was once considered extremely expensive and complex._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**X:**\n",
    "\n",
    "**Y:**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through all of the data, it looks like the Schedules DataFrame has all of the information we need to determine which games occurred on which dates and which teams won, as well as their home/visitor affiliation. Let's ignore this for now. Similarly, although the Season Player Stats DataFrame will likely be useful to us in the future, because it's a table of aggregate statistics there's not much we can do in terms of exploration right now. Instead, let's dive into the Box Scores DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box_scores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the columns, we see that there are a number of key stats. However, these stats are only available on an individual game-by-game basis. For our feature generation step, we may need this information aggregated across multiple games (but not at the season level). For instance, if we had a feature called `player_field_goal_percentage_to_date`, we would need to calculate the field goal percentage for a single player such as LeBron James for all of the games up to a certain date in the season.\n",
    "\n",
    "If the Cavaliers are playing the Golden State Warriors in December, then it might be useful to calculate LeBron's field goal percentage for the season up to the date of the upcoming game. (We can also compare this to his previous season stats, which we can pull from the Season Player Stats DataFrame.)\n",
    "\n",
    "Let's go ahead and try to do that! First, create a new column in Box Scores called `field_goal_percentage`. While you're at it, also create a new column called `full_name` that consists of the `first_name` and `last_name` fields concatenated with a space. Finally, convert the `date` column from type `str` to type `pd.Timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function called `get_player_FGP_to_date(df, player_full_name, date)`. In this function, create a pivot table on a date-filtered slice of your original data with `index` as `full_name` and `values` as the new `field_goal_percentage` column you created. Make sure that your function returns a float!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_player_FGP_to_date(df, player_full_name, date):\n",
    "    \"\"\"\n",
    "    player_full_name: str of format 'LeBron James'\n",
    "    date: str of format '2018-02-28' <== Cast this to pd.Timestamp using pd.to_datetime()\n",
    "    @return result: float between 0 and 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As a sanity check, the below cell should yield 0.55075._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_player_FGP_to_date(box_scores, 'LeBron James', '2014-12-25') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write another function `get_player_TPP_to_date(df, player_full_name, date)` that generates a similar pivot table but for `three_point_percentage`. As before, create this column and then write the function, ensuring that it returns a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_player_TPP_to_date(df, player_full_name, date):\n",
    "    \"\"\"\n",
    "    player_full_name: str of format 'LeBron James'\n",
    "    date: str of format '2018-02-28' <== Cast this to pd.Timestamp using pd.to_datetime()\n",
    "    @return result: float between 0 and 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As a sanity check, the below cell should yield 0.33232._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_player_TPP_to_date(box_scores, 'LeBron James', '2015-12-25') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive fully into feature generation, take a moment to think about one of the trickiest problems that we've had to deal with. Time traveling!\n",
    "\n",
    "For example, if we're trying to predict the box office gross of Inception, and one of our X variables is `average_director_lifetime_gross`, we can't use movies directed by Christopher Nolan that came out after Inception. We have to date-filter our DataFrame to only include movies before the release date!\n",
    "\n",
    "Similarly, if we want to calculate a player's three-point percentage to date, we can't include box scores from games that haven't happened yet. \n",
    "\n",
    "When we perform feature generation for each observation (each historical game), we will need to use functions such as the ones above to calculate all of our derived X variables. As you might expect, this will require us to loop through our DataFrame of games using `iterrows` or `apply`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feature generation\n",
    "\n",
    "Tune in next week! Take some time to think about the features that you would want to generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: \n",
    "### Created by: Joe\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gambling strategies\n",
    "\n",
    "Gambling strategies.  Let's use Pandas to go gambling.  Last time we did some data cleaning, but we didn't really get to using our data.  This time we'll alter a little of our data processing, and then actually use our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# damn pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in our data again:\n",
    "\n",
    "Easy removal of na_values using the na_values parameter:  Our bad values all have the string \"null\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in our data with more help from the read_csv parameters\n",
    "df = pd.read_csv(\"AAPL.csv\",na_values = [\"null\"])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only concerned with the Dates and Closing values, so get rid of the extra data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the unwanted columns\n",
    "df = \n",
    "\n",
    "# what does our data look like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to load in the Tesla and Netflix data.  Read the Tesla and Netflix Data in to df2 and df3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the tesla and netflix data, and pare down the columns\n",
    "df2 = \n",
    "df3 = \n",
    "\n",
    "# Pare down the unwanted columns\n",
    "df2 = \n",
    "df3 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load all of the datasets, merge them together keeping just the dates and the closing values.  Rename the closing values so that we can distinguish the columns (hint, see section notes from a few weeks ago):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the data together into one file, and change the column\n",
    "# names appropriately.\n",
    "df = \n",
    "df = \n",
    "\n",
    "# And change the column names to fit a reasonable mold\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The result of loading, paring down, and renaming columns should\n",
    "# look something like this:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we converted our dates to numbers, but this week we'll convert dates to time objects so we can use time deltas on them.  What do our times look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our \"Date\" looks like strings.\n",
    "df.loc[1:4,\"Date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime objects will let us more easily calculate differences in dates (as well as do power date manipulations like select particular months, days of the week, times of day, et cetera), so we'll convert our first column into date time objects.  We previously learned how to use the apply command for Series and data frames.  Use the apply command to change the dates in our dataframe to datetime objects.  Save the resulting series over the existing Date column.\n",
    "\n",
    "The func required for your apply command should use the datetime strptime function.  See the documentation for more on how to use strptime: https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n",
    "\n",
    "(If you have trouble with the strptime function, there is another method that uses a pandas library command which will let you infer date-times without knowing how strptime( ) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert your \"Date\" objects into datetime objects.\n",
    "df.loc[:,\"Date\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should read datetime64 for Date, and float for everything else.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have datetimes instead of strings, we can do easier calculations on them.  Create a mask of all the dates that were within the 10 days prior to March 31st, 2017 and show the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably want to create a datetime object for March 31st 2017\n",
    "date = datetime.datetime.strptime(   )\n",
    "\n",
    "# create a logical column with the datetime object and a timedelta object\n",
    "mask = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This command should output closing values for our three\n",
    "# stocks from March 22nd to March 30th.\n",
    "df.loc[mask,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function which finds the 25th and 75th percentiles as we did last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function takes a date, an index of dates in the form of a series,\n",
    "# a data frame with close data, the number of days to reference, and quantiles.\n",
    "# it returns the quantiles for the stocks in the data frame over the given\n",
    "# interval.  Only three need to be defined.  The data-frame shouldn't include\n",
    "# the dates, just the closing prices.\n",
    "def percents( date_time , date_series , data_frame , look_back = 365 , quantiles = [25,75]):\n",
    "    \n",
    "    # turn the look back integer into a timedelta\n",
    "    look_back = datetime.timedelta(look_back)\n",
    "\n",
    "    # create a mask of acceptable dates\n",
    "    mask = (-look_back < date_series - date_time) & (date_series - date_time <= datetime.timedelta(0))\n",
    "    \n",
    "    # find the percentile on the data that fits the range.  Axis = 0 allows runs the\n",
    "    # function over columns instead of rows.\n",
    "    nums = np.percentile( data_frame[mask],q = quantiles,axis = 0)\n",
    "    \n",
    "    # return the percentile calculation for each column of the data-frame.\n",
    "    return nums\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function that adds the quantiles to your data-frame for each of the stocks in our portfolio.  We've done this before, but this time, you can't use a for loop to iterate over the rows of the data if you want full credit.  apply( ) and loc( ) are the name of the game here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your function should take in a data frame, upper and lower quantiles,\n",
    "# and possibly the number of days to look back when calculating the\n",
    "# quantiles, and output a data frame with the quantiles added in successive\n",
    "# columns.  If column 0 is the dates, 1,2,3 are our stock closes, columns\n",
    "# 4,5,6 should be the quantiles for stocks 1,2,3.\n",
    "\n",
    "def add_percentiles(df, quantiles = [25,75], look_back = 365):\n",
    "    \n",
    "    data_frame = df.copy()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how does our result look?  Should get something like this.\n",
    "df1 = add_percentiles(df)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have some data to play with.  We're going to assess a betting strategy wherein we start with 1000 dollars.  If a stock we are following reaches the 75th percentile of the previous year's data, we'll sell half of our holdings.  If it reaches the 25th percentile, we'll buy stock according to half the cash we have on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes three stocks and applies our trading strategy\n",
    "# across the portfolio.  It assumes a 3% monetary inflation rate.\n",
    "# the return value is the value of the portfolio at each time \n",
    "# interval along the way.  The function assumes you have seven\n",
    "# columns in your data frame, and that the 0th is dates, 1-3\n",
    "# are stock names, and 4-6 are corresponding stock quantiles\n",
    "def values(process):\n",
    "    \n",
    "    # Assumed inflation rate\n",
    "    r = .03\n",
    "    buy = [\"neither\",\"neither\",\"neither\"]\n",
    "\n",
    "    then = process.iloc[0,0]\n",
    "\n",
    "    \n",
    "    portfolio = {\"cash\":1000,1:0,2:0,3:0}\n",
    "    values = [1000]\n",
    "\n",
    "    # values gives the rows of the data frame.\n",
    "    for row in process.values:\n",
    "        \n",
    "        # set the time\n",
    "        now = row[0]\n",
    "        \n",
    "        # grab the three stock close prices\n",
    "        one,two,three = row[1:4]\n",
    "        \n",
    "        # iterating over each pair of stocks:\n",
    "        for j in range(1,4):\n",
    "\n",
    "            # extract the current close,low/high for each stock in turn\n",
    "            close = row[j]\n",
    "            low,high = row[j+3]\n",
    "\n",
    "            # if the close is below the lower quantile, buy if we can buy\n",
    "            if close < low and buy[j-1] != \"bought\":\n",
    "\n",
    "                # find days passed\n",
    "                days = (now-then).days\n",
    "\n",
    "                # update the current\n",
    "                then = now\n",
    "\n",
    "                # update cash according to bank interest\n",
    "                cash = portfolio[\"cash\"] * np.exp(r*days/365)\n",
    "\n",
    "                # buy stock with half of cash\n",
    "                portfolio[j] += cash/2 / close\n",
    "\n",
    "                # update cash\n",
    "                portfolio[\"cash\"] = cash/2\n",
    "\n",
    "                buy[j-1] = \"bought\"\n",
    "\n",
    "            # sell if needed\n",
    "            elif close > high and buy[j-1] != \"sold\":\n",
    "\n",
    "                # find days passed\n",
    "                days = (now-then).days\n",
    "\n",
    "                # update the current\n",
    "                then = now\n",
    "\n",
    "                # update cash according to bank interest\n",
    "                cash = portfolio[\"cash\"] * np.exp(r*days/365)\n",
    "\n",
    "                # sell half of stock\n",
    "                portfolio[j] /= 2 \n",
    "\n",
    "                # update cash\n",
    "                portfolio[\"cash\"] += portfolio[j] * close\n",
    "\n",
    "                buy[j-1] = \"sold\"\n",
    "        \n",
    "        worth = portfolio[\"cash\"] + portfolio[1]*one + portfolio[2] * two + portfolio[3]*three\n",
    "        values.append(worth)\n",
    "        \n",
    "    return values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate stock buys over a period\n",
    "value1 = values(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much were we able to grow our money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what was the last entry in each of the portfolio values.  Did we make any money? \n",
    "value1[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a reasonable growth rate, although admittedly, not as good as could have been done with these three stocks.\n",
    "\n",
    "What you need to do now is pick different quantiles to try in our trading strategy.  We default to 25 and 75 as the percentiles, but you could pick extremes like 1 and 99, 49 and 51, or something assymetric like 45 and 99.\n",
    "\n",
    "Each set of quantiles you generate should give a new set of historic quantiles.  Take your best three sets of values and plot all the different sets on one line graph with plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating quantiles for our data with multiple different quantile settings\n",
    "df2 = add_percentiles(df,quantiles = )\n",
    "df3 = add_percentiles(df,quantiles = )\n",
    "\n",
    "df2.head()\n",
    "df3.head()\n",
    "\n",
    "value2 = values(df2)\n",
    "value3 = values(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the plotly library to generate visualizations for your exploraton.\n",
    "Generate a line graph with the data you've generated.  Use the plotly basic line.  Make sure to label your graph!\n",
    "\n",
    "(Reference for [plotly line-graphs](https://plot.ly/python/line-charts/#line-plot-modes).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First plot the graph of the [25,75] quantile portfolio results.\n",
    "# You'll need to do a little work getting reasonable x axis values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  One scatter plot.  Now plot again, but plot three different sets of quantiles, and label each accordingly.  We want to be able to see how our choices performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Like the above, but with three sets of data overlaid:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested our algorithm on a few stocks of my choosing, do the same for 3 stocks of your choosing.  Repeat the analysis, and leave a blurb about what you find.  Is it possible to use this algorithm to make money on stocks that have mostly level value or maybe even lose value?\n",
    "\n",
    "Note, it's really easy to get stock data from yahoo finance.  I got the apple data from [here](https://finance.yahoo.com/quote/AAPL/history?p=AAPL).  For a different stock, enter the company symbol of your choice.  You want historical data, and once you set the time frame, click the download data button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your stocks and analysis here:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
